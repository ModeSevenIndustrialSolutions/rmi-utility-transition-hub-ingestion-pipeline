{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41652f59-1798-4431-90dc-592dd4f64a7f",
   "metadata": {},
   "source": [
    "## Load RMI Utilities Transition Hub Data (from https://utilitytransitionhub.rmi.org/data-download/ for Data Vault Prototype)\n",
    "\n",
    "Copyright (C) 2021 OS-Climate\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "### We have a local copy rooted in the S3_BUCKET : RMI/data_download/*\n",
    "\n",
    "### The next step is to enrich with OS-C Factor metadata\n",
    "\n",
    "Contributed by Michael Tiemann (Github: MichaelTiemannOSC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92377eb7-1d1b-4662-ac08-99877153832b",
   "metadata": {},
   "source": [
    "Load Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac18bf3b-80d7-4b25-8ae4-9273709a0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the AWS Account page, copy the export scripts from the appropriate role using the \"Command Line or Programmatic Access\" link\n",
    "# Paste the copied text into ~/credentials.env\n",
    "\n",
    "from dotenv import dotenv_values, load_dotenv\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "dotenv_dir = os.environ.get('CREDENTIAL_DOTENV_DIR', os.environ.get('PWD', '/opt/app-root/src'))\n",
    "dotenv_path = pathlib.Path(dotenv_dir) / 'credentials.env'\n",
    "if os.path.exists(dotenv_path):\n",
    "    load_dotenv(dotenv_path=dotenv_path,override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5877739-a3c7-4f6c-ae92-b2288f511b41",
   "metadata": {},
   "source": [
    "Create an S3 resource for the bucket holding source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3207a36b-b6ec-4694-841b-160a13a8a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_resource = boto3.resource(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ['S3_LANDING_ENDPOINT'],\n",
    "    aws_access_key_id=os.environ['S3_LANDING_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_LANDING_SECRET_KEY'],\n",
    ")\n",
    "bucket = s3_resource.Bucket(os.environ['S3_LANDING_BUCKET'])\n",
    "\n",
    "rmi_folder = bucket.Object('RMI/data_download')\n",
    "\n",
    "files = []\n",
    "for file in bucket.objects.filter(Prefix='RMI/data_download'):\n",
    "    if file.key.endswith('csv'):\n",
    "        files.append(file.key)\n",
    "files[0:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bee29e-72ec-4fbc-b400-daa946277b67",
   "metadata": {},
   "source": [
    "Build a map and define schema mapping logic for parquet to sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22285cd0-a545-4a3b-823b-27ff40555080",
   "metadata": {},
   "outputs": [],
   "source": [
    "_p2smap = {\n",
    "    'object': 'varchar',\n",
    "    'string': 'varchar',\n",
    "    'str': 'varchar',\n",
    "    'float32': 'real',\n",
    "    'Float32': 'real',\n",
    "    'float64': 'double',\n",
    "    'Float64': 'double',\n",
    "    'int32': 'integer',\n",
    "    'Int32': 'integer',\n",
    "    'int64': 'bigint',\n",
    "    'Int64': 'bigint',\n",
    "    'category': 'varchar',\n",
    "    'datetime64[ns, UTC]': 'timestamp',\n",
    "    'datetime64[ns]': 'timestamp'\n",
    "}\n",
    "\n",
    "def pandas_type_to_sql(pt):\n",
    "    st = _p2smap.get(pt)\n",
    "    if st is not None:\n",
    "        return st\n",
    "    raise ValueError(\"unexpected pandas column type '{pt}'\".format(pt=pt))\n",
    "\n",
    "# add ability to specify optional dict for specific fields?\n",
    "# if column name is present, use specified value?\n",
    "def generate_table_schema_pairs(df):\n",
    "    ptypes = [str(e) for e in df.dtypes.to_list()]\n",
    "    stypes = [pandas_type_to_sql(e) for e in ptypes]\n",
    "    pz = list(zip(df.columns.to_list(), stypes))\n",
    "    return \",\\n\".join([\"    {n} {t}\".format(n=e[0],t=e[1]) for e in pz])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c700b7b-e370-4fe0-95b9-3c8104ce822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trino\n",
    "\n",
    "conn = trino.dbapi.connect(\n",
    "    host=os.environ['TRINO_HOST'],\n",
    "    port=int(os.environ['TRINO_PORT']),\n",
    "    user=os.environ['TRINO_USER'],\n",
    "    http_scheme='https',\n",
    "    auth=trino.auth.JWTAuthentication(os.environ['TRINO_PASSWD']),\n",
    "    verify=True,\n",
    ")\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c831c4-d3ad-4a28-8127-de5e4047e775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available schemas to ensure trino connection is set correctly\n",
    "cur.execute('show schemas in osc_datacommons_dev')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effb6763-7b85-43ff-b073-97f2a9ac6ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaname = 'rmi_utility_transition_hub'\n",
    "cur.execute('create schema if not exists osc_datacommons_dev.' + schemaname)\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7eab8e-10be-4fa1-b9a2-05006bbaa9ac",
   "metadata": {},
   "source": [
    "Load RMI data file using pandas *read_csv* and appropriate dtype dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e092d951-37d5-429b-955c-2ace5b1f3650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dtype_dict = {\n",
    "    # Table Name maps to dtypes (note that NaNs cannot encode to integers, so must do ex post facto fixes)\n",
    "    'assets_earnings_investments':\n",
    "        {'respondent_id':'int32', 'year':'int32',\n",
    "         'asset_value':'float64', 'earnings_value':'float64', 'investment_value':'float64'},\n",
    "    'customers_sales':\n",
    "        {'respondent_id':'int32', 'year':'int32',\n",
    "         'sales':'float64', 'revenues':'float64'},\n",
    "    'debt_equity_returns':\n",
    "        {'respondent_id':'int32', 'year':'int32',\n",
    "         'rate_base_actual':'float64', 'equity_actual':'float64', 'debt_actual':'float64', \n",
    "         'equity_ratio_actual':'float64', 'returns_actual':'float64', 'earnings_actual':'float64',\n",
    "         'interest_actual':'float64', 'fed_tax_expense_actual':'float64',\n",
    "         'pre_tax_net_income_actual':'float64', 'ROR_actual':'float64', 'ROE_actual':'float64', \n",
    "         'interest_rate':'float64',\n",
    "         'effective_fed_tax_rate':'float64', 'equity_authorized':'float64', 'debt_authorized':'float64',\n",
    "         'returns_authorized':'float64', 'earnings_authorized':'float64', 'interest_authorized':'float64', \n",
    "         'interest_rate_authorized':'float64'},\n",
    "    'emissions_targets':\n",
    "        { # Dirty data prevents this up-front: 'year':'int32',\n",
    "         'CO2_historical':'float64', 'CO2_target':'float64', 'CO2_target_all_years':'float64', 'CO2_1point5C':'float64',\n",
    "         'generation_historical':'float64', 'generation_projected':'float64', 'generation_1point5C':'float64',\n",
    "         'CO2_intensity_historical':'float64', 'CO2_intensity_target':'float64', 'CO2_intensity_target_all_years':'float64', 'CO2_intensity_1point5C':'float64'},\n",
    "    'expenditure_bills_burden':\n",
    "        {'respondent_id':'int32', 'year':'int32',\n",
    "         'expenditure':'float64', 'bill':'float64', 'burden':'float64'},\n",
    "    'housing_units_income':\n",
    "        {'respondent_id':'int32', 'year':'int32',\n",
    "         'housing_units':'float64', 'income':'float64'},\n",
    "    'net_plant_balance':\n",
    "        {'respondent_id':'int32', 'year':'int32',\n",
    "         'original_cost':'float64', 'accum_depr':'float64', 'net_plant_balance':'float64',\n",
    "         'ARC':'float64', 'ARC_accum_depr':'float64', 'net_ARC':'float64'},\n",
    "    'operations_emissions_by_fuel':\n",
    "        {'respondent_id':'int32', 'year':'int32',\n",
    "         'latitude':'float64', 'longitude':'float64',\n",
    "         # Dirty data prevents this up-front: 'operating_month':'int32', 'operating_year':'int32',\n",
    "         'capacity':'float64', 'year_end_capacity':'float64', 'generation':'float64', 'potential_generation':'float64'},\n",
    "    'operations_emissions_by_tech':\n",
    "        {'respondent_id':'int32', 'year':'int32',\n",
    "         'latitude':'float64', 'longitude':'float64',\n",
    "         'capacity':'float64', 'year_end_capacity':'float64', 'generation':'float64', 'potential_generation':'float64'},\n",
    "    'revenue_by_tech':\n",
    "        {'respondent_id':'int32', 'year':'int32', 'latitude':'float64', 'longitude':'float64',\n",
    "         'capacity':'float64', 'year_end_capacity':'float64', 'generation':'float64', 'potential_generation':'float64'},\n",
    "    'state_targets': str,\n",
    "    'utility_information': {'respondent_id':'int32'},\n",
    "    'utility_state_map': {'respondent_id':'int32'}\n",
    "}\n",
    "\n",
    "fillna_dict = {\n",
    "    'assets_earnings_investments':\n",
    "        {'asset_value': 0, 'earnings_value': 0, 'investment_value': 0},\n",
    "    'customer_sales':\n",
    "        {'customers': 0, 'sales': 0, 'revenues': 0}\n",
    "}\n",
    "\n",
    "dropna_dict = {\n",
    "    'emissions_targets': {'respondent_id':'int32'},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c4cd86-3a3c-4cb0-a0b9-7a8b8c866776",
   "metadata": {},
   "outputs": [],
   "source": [
    "tablename_to_df = {}\n",
    "\n",
    "def create_trino_pipeline (s3, schemaname, tablename, df):\n",
    "    global tablename_to_df\n",
    "    df.to_parquet('/tmp/{sname}.{tname}.parquet'.format(sname=schemaname, tname=tablename), index=False)\n",
    "    tablename_to_df[tablename] = df\n",
    "    s3.upload_file(\n",
    "        Bucket=os.environ['S3_DEV_BUCKET'],\n",
    "        Key='trino/{sname}/{tname}/{tname}.parquet'.format(sname=schemaname, tname=tablename),\n",
    "        Filename='/tmp/{sname}.{tname}.parquet'.format(sname=schemaname, tname=tablename)\n",
    "    )\n",
    "    cur.execute('.'.join(['drop table if exists osc_datacommons_dev', schemaname, tablename]))\n",
    "    cur.fetchall()\n",
    "    \n",
    "    schema = generate_table_schema_pairs(df)\n",
    "\n",
    "    tabledef = \"\"\"create table if not exists osc_datacommons_dev.{sname}.{tname}(\n",
    "{schema}\n",
    ") with (\n",
    "    format = 'parquet',\n",
    "    external_location = 's3a://{bucket}/trino/{sname}/{tname}/'\n",
    ")\"\"\".format(schema=schema,bucket=os.environ['S3_DEV_BUCKET'],sname=schemaname,tname=tablename)\n",
    "    print(tabledef)\n",
    "\n",
    "    # tables created externally may not show up immediately in cloud-beaver\n",
    "    cur.execute(tabledef)\n",
    "    cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1db2f9-7bed-4b81-866e-27e2f4ed8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an S3 client\n",
    "s3 = boto3.client(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ['S3_DEV_ENDPOINT'],\n",
    "    aws_access_key_id=os.environ['S3_DEV_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_DEV_SECRET_KEY'],\n",
    ")\n",
    "\n",
    "df_nn = None\n",
    "\n",
    "for f in files:\n",
    "    tablename = f.split('/')[-1].split('.')[0]\n",
    "    print(f)\n",
    "    if tablename=='state_utility_policies':\n",
    "        df = pd.read_csv(bucket.Object(f).get()['Body'],\n",
    "                         dtype={'respondent_id':'int32'},parse_dates=['date_updated'],dayfirst=True)\n",
    "    else:\n",
    "        df = pd.read_csv(bucket.Object(f).get()['Body'], dtype=dtype_dict[tablename])\n",
    "    if tablename in dropna_dict:\n",
    "        df.dropna(subset=list(dropna_dict[tablename].keys()), inplace=True)\n",
    "    if tablename in fillna_dict:\n",
    "        df.fillna(value=fillna_dict[tablename], inplace=True)\n",
    "    \n",
    "    # This code relies on seeing 'operations_emissions_by_fuel' before 'operations_emissions_by_tech'\n",
    "    if tablename in ['operations_emissions_by_fuel', 'operations_emissions_by_tech']:\n",
    "        # Both tables duplicate the 'Purchased Power' and 'EE & DR' data.\n",
    "        # We only need one copy, which we create as 'other_generation'\n",
    "        if tablename=='operations_emissions_by_fuel':\n",
    "            df_anon = df[df['plant_name_eia'].isna()]\n",
    "            # Drop many NULL columns we don't need\n",
    "            df_anon.dropna(axis=1, how='all', inplace=True)\n",
    "            create_trino_pipeline (s3, schemaname, 'other_generation', df_anon)\n",
    "            df_nn = df[~df['operating_month'].isna()]\n",
    "        df.dropna(subset=['plant_name_eia'], inplace=True)\n",
    "        # For some reason, data before 2010 is sometimes note filled in.  This \n",
    "        for index, row in df[df['operating_month'].isna()].iterrows():\n",
    "            # df_nn is only computed once, from 'operations_emissions_by_fuel'\n",
    "            df0 = df_nn[(df_nn['respondent_id']==row['respondent_id']) & (df_nn['generator_id']==row['generator_id'])][['operating_month', 'operating_year']]\n",
    "            if len(df0)==0:\n",
    "                # In this case we have no prior data to refer to\n",
    "                continue\n",
    "            om, oy = df0.iloc[0]\n",
    "            df.loc[index, ('operating_month', 'operating_year')] = om, oy\n",
    "        for colname in ['operating_month', 'operating_year']:\n",
    "            # df[colname] = pd.to_numeric(df[colname],downcast='int32')\n",
    "            pass\n",
    "    # if tablename in tidy_dict:\n",
    "    #     tidy_df = df.melt(id_vars=tidy_dict[tablename][0], value_vars=tidy_dict[tablename][2],\n",
    "    #                       var_name=tidy_dict[tablename][1][0], value_name=tidy_dict[tablename][1][1])\n",
    "    #     tidy_df[tidy_dict[tablename][1][0]] = tidy_df[tidy_dict[tablename][1][0]].apply(lambda x: x.split('_')[0])\n",
    "    #     tidy_df.dropna(subset=[tidy_dict[tablename][1][1]],inplace=True)\n",
    "    create_trino_pipeline (s3, schemaname, tablename, df)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fde1a9-b84d-4d16-a89c-38d939ab71a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load metadata following an ingestion process into trino metadata store\n",
    "\n",
    "### The schema is *metastore*, and the table names are *meta_schema*, *meta_table*, *meta_field*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4f8eed-d7ec-4444-ada6-8955c684bca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metastore = 'metastore'\n",
    "\n",
    "# Create a metadata schema with tables for the three layers of metadata: schema, table, and field.\n",
    "\n",
    "meta_schema = 'meta_schema'\n",
    "meta_table = 'meta_table'\n",
    "meta_field = 'meta_field'\n",
    "\n",
    "# These metadata tables are local to this ingestion process.\n",
    "# We will insert/merge with master metadata tables later\n",
    "\n",
    "metadata_to_df = {\n",
    "    # For each data source there is a single entry in the _schema_table\n",
    "    meta_schema: pd.DataFrame(data=[], columns=[]),\n",
    "    # For each data source there are one or more tables in the _tables_table\n",
    "    meta_table: pd.DataFrame(data=[],\n",
    "                    columns=['tname', 'parent_schema', 'source', 'processing_pipeline']),\n",
    "    # For each table there are one or more fields in the fields_table\n",
    "    meta_field: pd.DataFrame(data=[],\n",
    "                    columns=['fname', 'parent_table', 'type', 'dimension', 'description'])\n",
    "}\n",
    "\n",
    "cur.execute('create schema if not exists osc_datacommons_dev.' + metastore)\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5a3a84-c17d-4e8a-865f-59633ca3b97a",
   "metadata": {},
   "source": [
    "Create the actual metadata for the source.  In this case, it is *rmi_utility_transition_hub*\n",
    "\n",
    "We read and interpret the data dictionary that comes with the data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125262d3-1725-4c5d-a22f-f55c5d505d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "rmi_dd_obj = None\n",
    "for rmi_dd_obj in bucket.objects.filter(Prefix='RMI/data_download/RMI Utility Transition Hub Data Dictionary.xlsx'):\n",
    "    break\n",
    "\n",
    "rmi_dd_bytes = io.BytesIO(bucket.Object(rmi_dd_obj.key).get()['Body'].read())\n",
    "rmi_dd = pd.read_excel(rmi_dd_bytes, sheet_name='Overview', dtype=str)\n",
    "# Drop empty left column\n",
    "rmi_dd.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "title = rmi_dd.loc[0, 'Unnamed: 1']\n",
    "general_overview = rmi_dd.loc[1, 'Unnamed: 2']\n",
    "scope = rmi_dd.loc[2, 'Unnamed: 2']\n",
    "limitations_to_scope = rmi_dd.loc[3, 'Unnamed: 2']\n",
    "# Drop non-table data captured above\n",
    "rmi_dd.drop(list(range(0,5)), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3022d-a2b3-413d-bb17-e7633c14a906",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_mds_df = metadata_to_df[meta_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcad0d2-b860-4455-b6f2-31af3437c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "_mds_df['title'] = title\n",
    "_mds_df['description'] = general_overview\n",
    "_mds_df['version'] = 'Released in September 2021'\n",
    "_mds_df['uri'] = 'https://utilitytransitionhub.rmi.org/data-download/'\n",
    "_mds_df['copyright'] = '© 2021 RMI'\n",
    "_mds_df['license'] = 'Creative Commons Attribution-Noncommercial 4.0 International Public License (CC BY-NC)'\n",
    "_mds_df['contact'] = 'utilitytransitionhub@rmi.org'\n",
    "_mds_df['abstract'] = '\\n'.join([scope, limitations_to_scope])\n",
    "_mds_df['name'] = 'rmi_utility_transition_hub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cc5cc8-e8d9-474a-865a-1411dcd30d99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata_to_df[meta_schema] = _mds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94c6db3-be25-4051-b8d8-4690cc3b9608",
   "metadata": {},
   "source": [
    "Iterate through tablenames until we get to *Additional Information*, storing all the names and descriptions of the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90ebc13-eec6-45e8-992e-b70abe3386d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_mdt_df = metadata_to_df[meta_table]\n",
    "_mdt_df['tname'] = pd.Series(tablename_to_df.keys(), dtype=object)\n",
    "_mdt_df['parent_schema'] = pd.Series([ meta_schema ] * len(_mdt_df['tname']), dtype=object)\n",
    "_mdt_df['description'] = pd.Series([ '' ] * len(_mdt_df['tname']), dtype=object)\n",
    "\n",
    "metadata_to_df[meta_table] = _mdt_df\n",
    "\n",
    "for tablename, description in rmi_dd.itertuples(index=False):\n",
    "    if tablename == 'Additional Information':\n",
    "        break\n",
    "    _mdt_df.loc[_mdt_df['tname']==tablename, 'description'] = description\n",
    "\n",
    "# Add in this one table we generate ourselves from the _generation tables\n",
    "_mdt_df.loc[_mdt_df['tname']=='other_generation'] = 'EE & DR as well as Purchased Power, all of which count toward `avoided generation`'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a956dbf4-e5a7-46c0-bb91-ca8fcaca70f4",
   "metadata": {},
   "source": [
    "We've already collected the field names, types, dimensions, etc., of each table into our dataframes.\n",
    "But there's additional metadata we can collect from the data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f99372e-857e-4e67-ad0e-038541dffa70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_mdf_df = metadata_to_df[meta_field]\n",
    "\n",
    "import math\n",
    "units_dict = {\n",
    "    '$': 'USD',\n",
    "    'number': ''\n",
    "}\n",
    "\n",
    "for tablename, df in tablename_to_df.items():\n",
    "    # Initialize field metadata from dataframes of tables we have written out\n",
    "    if _mdf_df['fname'].empty==False:\n",
    "        new_df = pd.DataFrame()\n",
    "        new_df['fname'] = pd.Series(list(df.columns))\n",
    "        new_df['parent_table'] = pd.Series([tablename] * len(df.columns))\n",
    "        new_df['type'] = pd.Series([str(t) for t in df.dtypes])\n",
    "        _mdf_df = _mdf_df.append(new_df, ignore_index=True)\n",
    "    else:\n",
    "        _mdf_df['fname'] = pd.Series(df.columns)\n",
    "        _mdf_df['parent_table'] = pd.Series([tablename] * len(df.columns))\n",
    "        _mdf_df['type'] = pd.Series([str(t) for t in df.dtypes])\n",
    "    # Flesh out additional info from data dictionary\n",
    "    if tablename == 'other_generation':\n",
    "        # We have to enter this metadata by hand (later) for the tidy table we created\n",
    "        continue\n",
    "    dd_df = pd.read_excel(rmi_dd_bytes, sheet_name=tablename, dtype=str)\n",
    "    # Drop empty left column\n",
    "    dd_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "    dd_df.set_axis(['Data Field'] + list(dd_df.columns[1:-4]) + ['Definition', 'Units', 'Data Source', 'Methodology'], axis=1,inplace=True)\n",
    "    # Data Field; (sub1; (sub2;)) Definition; Units; Data Source; Methodology\n",
    "    for field, definition, units in dd_df[['Data Field', 'Definition', 'Units']].dropna(subset=['Data Field']).itertuples(index=False):\n",
    "        _mdf_df.loc[(_mdf_df['parent_table']==tablename) & (_mdf_df['fname']==field), 'description'] = definition\n",
    "        if type(units)==str:\n",
    "            if units in units_dict:\n",
    "                units = units_dict[units]\n",
    "            _mdf_df.loc[(_mdf_df['parent_table']==tablename) & (_mdf_df['fname']==field), 'dimension'] = units\n",
    "\n",
    "# Now add descriptions for *other_generation*\n",
    "for field, description in _mdf_df.loc[_mdf_df['parent_table']=='operations_emissions_by_fuel', ['fname', 'description']].dropna(subset=['description']).itertuples(index=False):\n",
    "    _mdf_df.loc[(_mdf_df['parent_table']=='other_generation') & (_mdf_df['fname']==field), 'description'] = description\n",
    "\n",
    "metadata_to_df[meta_field] = _mdf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a970546-8822-430a-94a2-736ecb6d724c",
   "metadata": {},
   "source": [
    "### The following is logically wrong because we're just slamming data into the metastore.  Next step is to update/insert data in case we are not the first/only ingestion process ever in the whole universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5d12f7-4217-4bf8-8e9c-7105824842c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tablename, df in metadata_to_df.items():\n",
    "    schema = generate_table_schema_pairs(df)\n",
    "    tabledef = \"\"\"create table if not exists osc_datacommons_dev.{sname}.{tname}(\n",
    "{schema}\n",
    ") with (\n",
    "    format = 'parquet',\n",
    "    external_location = 's3a://{bucket}/trino/{sname}/{tname}/'\n",
    ")\"\"\".format(schema=schema,bucket=os.environ['S3_DEV_BUCKET'],sname=metastore,tname=tablename)\n",
    "    print(tabledef)\n",
    "    \n",
    "    cur.execute(tabledef)\n",
    "    cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a321023-4fa1-434b-b244-80f5ec70bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_to_df[meta_field].dropna(subset=['dimension'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14118a97-3845-44f4-b491-620d0dde6ea8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
